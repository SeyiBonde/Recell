{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2r5o8SR4rqH"
   },
   "source": [
    "# Supervised Learning - Foundations Project: ReCell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SntBY974rqJ"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_GXJwuhmVz"
   },
   "source": [
    "### Business Context\n",
    "\n",
    "Buying and selling used phones and tablets used to be something that happened on a handful of online marketplace sites. But the used and refurbished device market has grown considerably over the past decade, and a new IDC (International Data Corporation) forecast predicts that the used phone market would be worth \\\\$52.7bn by 2023 with a compound annual growth rate (CAGR) of 13.6% from 2018 to 2023. This growth can be attributed to an uptick in demand for used phones and tablets that offer considerable savings compared with new models.\n",
    "\n",
    "Refurbished and used devices continue to provide cost-effective alternatives to both consumers and businesses that are looking to save money when purchasing one. There are plenty of other benefits associated with the used device market. Used and refurbished devices can be sold with warranties and can also be insured with proof of purchase. Third-party vendors/platforms, such as Verizon, Amazon, etc., provide attractive offers to customers for refurbished devices. Maximizing the longevity of devices through second-hand trade also reduces their environmental impact and helps in recycling and reducing waste. The impact of the COVID-19 outbreak may further boost this segment as consumers cut back on discretionary spending and buy phones and tablets only for immediate needs.\n",
    "\n",
    " \n",
    "### Objective\n",
    "\n",
    "The rising potential of this comparatively under-the-radar market fuels the need for an ML-based solution to develop a dynamic pricing strategy for used and refurbished devices. ReCell, a startup aiming to tap the potential in this market, has hired you as a data scientist. They want you to analyze the data provided and build a linear regression model to predict the price of a used phone/tablet and identify factors that significantly influence it.\n",
    "\n",
    " \n",
    "### Data Description\n",
    "\n",
    "The data contains the different attributes of used/refurbished phones and tablets. The data was collected in the year 2021. The detailed data dictionary is given below.\n",
    "\n",
    "\n",
    "- brand_name: Name of manufacturing brand\n",
    "- os: OS on which the device runs\n",
    "- screen_size: Size of the screen in cm\n",
    "- 4g: Whether 4G is available or not\n",
    "- 5g: Whether 5G is available or not\n",
    "- main_camera_mp: Resolution of the rear camera in megapixels\n",
    "- selfie_camera_mp: Resolution of the front camera in megapixels\n",
    "- int_memory: Amount of internal memory (ROM) in GB\n",
    "- ram: Amount of RAM in GB\n",
    "- battery: Energy capacity of the device battery in mAh\n",
    "- weight: Weight of the device in grams\n",
    "- release_year: Year when the device model was released\n",
    "- days_used: Number of days the used/refurbished device has been used\n",
    "- normalized_new_price: Normalized price of a new device of the same model in euros\n",
    "- normalized_used_price: Normalized price of the used/refurbished device in euros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_-uuGqH-qTt"
   },
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeF8YaNKDPyK"
   },
   "outputs": [],
   "source": [
    "#for making the Python code more structured automatically (good coding practice)\n",
    "%load_ext nb_black\n",
    "\n",
    "#to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#for splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for building linear regression_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#for checking model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "#for building linear regression_model using statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#to compute VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxhpZv9y-qTw"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJwX9wuc4rqL"
   },
   "outputs": [],
   "source": [
    "#loading the csv file\n",
    "df=pd.read_csv('used_device_data.csv')\n",
    "#Checking the first few lines of our data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvpMDcaaMKtI"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIiCRwqZ54_C"
   },
   "source": [
    "- Observations\n",
    "- Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01hJQ7EfMKtK"
   },
   "outputs": [],
   "source": [
    "#getting an overview of the data set\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. There are 9 float columns, 4 object/string columns and 2 integer columns in this data set.\n",
    "2. Brand name, os, 4g and 5g are categorical while the others are numerical\n",
    "3. some of the columns have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has 3454 rows and 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the statistical details of the data set\n",
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. There are 34 unique brands\n",
    "2. Android OS is the most popular OS\n",
    "3. The largest Main camera has 48mega pixels and the smallest main camera has 0.08 mp\n",
    "4. The largest selfie camera has 32 mega pixelsFor example, the highest normalized used price us 7.848.\n",
    "5. The mean new price is 5.233 and the mean used price is 4.36\n",
    "6. The average for most of the numerical data is larger than the median which may mean we have skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in the data set\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are no duplicates in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values in the data set\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of the data set to prevent tampering with the original\n",
    "df1=df.copy()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are 202 null values in the data set which we will be treating soon. the main_camera_mp column has the highest number of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__7ciGcIDPyk"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the distributions for all the variables\n",
    "#plotting histograms for all the columns in the data set using a for loop\n",
    "for i in df1.columns:\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.histplot(data=df1,x=i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a box plot for all the numerical variables in the data set\n",
    "num_cols = df1.select_dtypes(include=np.number).columns.tolist()\n",
    "for i, variable in enumerate (num_cols):\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.boxplot(data=df1,x=variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the Univariate analysis\n",
    "\n",
    "From the Histograms and box plots of all the columns, we can observe the following:\n",
    "\n",
    "1. The brand name 'Others\" has the most counts, however, other is most likely a mixture of many brands. The Single brand name that has the highest number of phones and tablet is Samsung, and the least is Infinix.\n",
    "2. Most of the phones and tablets use the Android OS while a minority uses the IOS\n",
    "3. The most common screen size amongst the phones and tablets is between 11-13cm\n",
    "4. Most of the phones and tablets have 4g while a few do not.\n",
    "5. only a few phones and tablets are 5g enabled. most are not\n",
    "6. Most of the phones have main camera mp of 13mp\n",
    "7. Most of the phones have selfie camera mp of 5\n",
    "8. Phones with Internal memory below 200 have the most count. only a few have higher than 200\n",
    "9. 4gb Ram is the most common ram for phones and tablets in this data set\n",
    "10. batteries larger than 6000mah are quite rare. majority of the phones and tabs have batteries 4500mah and below\n",
    "11. In this data set, phones released in 2014 are the most dominant in terms of count, and it's closely followed by phones released in 2013\n",
    "12. From the plot for days used, we see that most of the phones and tablets in this data set have been used for more than 500 days. this may mean that most people are willing to sell their phones after they have enjoyed it for at least 500 days.\n",
    "13. The normalized used price plot has a fairly normal distribution, while the normalized new price looks like it has a binormial distribution\n",
    "14. the box plots show the outliers in some columns of this data set and this helps us decide how to fill the null values\n",
    "15. From the histogram, we can see that the distribution for weight is right skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a box plot of normalized used price and OS to observe the relationship\n",
    "sns.boxplot(data= df1, x='os',y= 'normalized_used_price')\n",
    "plt.axhline(df1['normalized_used_price'].mean(), color=\"gray\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a box plot of normalized used price and 5g\n",
    "sns.boxplot(data= df1, x='5g',y= 'normalized_used_price')\n",
    "plt.axhline(df1['normalized_used_price'].mean(), color=\"gray\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a box plot of normalized used price and 4g\n",
    "sns.boxplot(data= df1, x='4g',y= 'normalized_used_price')\n",
    "plt.axhline(df1['normalized_used_price'].mean(), color=\"gray\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a barplot of normalized used price and brand_name\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(data= df, x='brand_name',y= 'normalized_used_price',ci=False)\n",
    "plt.axhline(df1['normalized_used_price'].mean(), color=\"gray\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lineplot of normalized used price and release_year\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.lineplot(data= df, x='release_year',y= 'normalized_used_price',ci=False)\n",
    "plt.axhline(df1['normalized_used_price'].mean(), color=\"gray\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the bivariate analysis:\n",
    "\n",
    "1. Ios has the highest normalized_used price, followed by the android os.\n",
    "2. Normalized used price for phones with 5g is more than those without. phones without 5g cost below average. All phones and tablets that have 5g cost more than average.\n",
    "3. Normalized used price for phones with 4g is also more than those without. phones with 4g cost slightly above average while  75% of the devices without 4g cost below the average normalized used price.\n",
    "4. The normalized used price of oneplus phones and tablets is the highest, followed by apple. On average, about 14 brand names have normalized used price above the average.\n",
    "5. From the line plot, we can see that as the year increases, the normalized use price increases. telling us that more recent devices has a higher used price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bGVKmh75ri8"
   },
   "source": [
    "- EDA is an important part of any project involving data.\n",
    "- It is important to investigate and understand the data better before building a model with it.\n",
    "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
    "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEyqzdJBb0jU"
   },
   "source": [
    "**Questions**:\n",
    "\n",
    "1. What does the distribution of normalized used device prices look like?\n",
    "2. What percentage of the used device market is dominated by Android devices?\n",
    "3. The amount of RAM is important for the smooth functioning of a device. How does the amount of RAM vary with the brand?\n",
    "4. A large battery often increases a device's weight, making it feel uncomfortable in the hands. How does the weight vary for phones and tablets offering large batteries (more than 4500 mAh)?\n",
    "5. Bigger screens are desirable for entertainment purposes as they offer a better viewing experience. How many phones and tablets are available across different brands with a screen size larger than 6 inches?\n",
    "6. A lot of devices nowadays offer great selfie cameras, allowing us to capture our favorite moments with loved ones. What is the distribution of devices offering greater than 8MP selfie cameras across brands?\n",
    "7. Which attributes are highly correlated with the normalized price of a used device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a histogram to show the distribution of normalized used prices\n",
    "sns.histplot(df1.normalized_used_price);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a boxplot for normalized used prices\n",
    "sns.boxplot(data=df1, x='normalized_used_price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "1. From the histplot for normalized used prices for devices, we can observe a normal distribution though slightly left skewed.\n",
    "2. from the boxplot, we can observe that 75% of the phones and tablets have a normalized used price below 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a histplot showing the OS distribution\n",
    "sns.histplot(df1.os);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the percentage of phones and tablets that have android OS\n",
    "df1.os.value_counts('Android')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "approximately 93.1% of the phones and tablets use android OS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how the amount of Ram varies with each brand by plotting a bar grph\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(y=df1.ram, x=df1.brand_name, ci=False);\n",
    "plt.xticks(rotation=90)\n",
    "plt.axhline(df1['ram'].mean(), linestyle='dashed')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how the amount of Ram varies with each brand by plotting a box plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(y=df1.ram, x=df1.brand_name);\n",
    "plt.xticks(rotation=90)\n",
    "plt.axhline(df1['ram'].mean(), linestyle='dashed')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "The brand with the highest ram is Oneplus, and the brand with the lowest ram is Celkon. The brand names with ram above average are Honor, Huawei,Meizu,Oneplus, Oppo,Realme,Samsung,Vivo,Xiaomi and google. Sony has an average Ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking all devices with battery larger than 4500\n",
    "df1[df1.battery > 4500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "There are 341 devices with batteries above 4500. I will make them a new data set and plot a bar plot with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data frame for batteries above 4500\n",
    "batt_above_4500= df1[df1.battery > 4500]\n",
    "#creating barplot for the large battery data frame\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(data=batt_above_4500,x='brand_name', y='weight', ci=False)\n",
    "plt.axhline(batt_above_4500.weight.mean(), linestyle='dashed')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating boxplot for the large battery data frame\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.boxplot(data= batt_above_4500, x= 'brand_name',y= 'weight')\n",
    "plt.axhline(batt_above_4500.weight.mean(), linestyle='dashed')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. The google devices that have batteries larger than 4500 weigh the most. \n",
    "2. the Micromax brand hasthe least weight for devices with batteries larger than 4500. \n",
    "3. about 11 brands with devices having battery of more than 4500 weigh above the average weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the size of the screen given in the data set is actually in cm and not inches,i need to do a conversion\n",
    "#since 6 inches = 15.24cm, i will be using 15.24 in the calculation\n",
    "df1[df1.screen_size>15.24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are 1099 Phones and tablets that have screen size greater than 6\". I'll plot them on a graph to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame for screen above 6 inches:\n",
    "screen_above_6=df1[df1.screen_size>15.24]\n",
    "#plotting a hist plot for new data frame\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(screen_above_6.brand_name,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations: \n",
    "Huawei has the highest numer of phones and tablets with screen above 6 inches. samsung,vivo, others amd Honor have quite some options with screens above 6 inches as well. is Huawei. brands like karbonn, Microsoft, coolpad, google and gionee have only a few phones and tablets that have larger screens than 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting devices with selfie camera above 8\n",
    "#I will plot this in a graph without necessarily creating a new data frame by making the x axis pass only the brand_names with selfie camera larger than 8 in the plot.\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(data=df1, x=df1[df1.selfie_camera_mp>8.0].brand_name);\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "Customers who enjoy taking  selfies would prefer to choose from brand names like Huawei,Vivo, Oppo and Xiaomi. there are very few options for phones and tablets with front camera larger tha from Infinix, Acer, Blackberry, Micromax and Panasonic,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the correlation between all numerical variables\n",
    "cols_list = df1.select_dtypes(include=np.number).columns.tolist()\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(df1[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. The variable that has the strongest positive correlation with the normalized used price is the normalized new price.\n",
    "\n",
    "2. We can also observe positive correlationg between normalized used price and release year, battery, ram,selfie camera mp, main camera mp and screen size.\n",
    "\n",
    "3. We can also observe weak positive correlations bewtween the normalized used price and the internal memory, and weight.\n",
    "\n",
    "4. we may not pay much attention to the release year because it's value keeps varying.\n",
    "\n",
    "5. When we look closely, we will also observe some strong correlations between some columns:\n",
    "\n",
    "6. Battery and weight has a strong correlation with screen size and even the weight and battery are correlated.\n",
    "\n",
    "7. we can also see correlations between the selfie camera and release year, which also makes sense of the negative correlation between days used and selfie camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVn5toJ7MKte"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcceZiPd5vGV"
   },
   "source": [
    "- Missing value treatment\n",
    "- Feature engineering (if needed)\n",
    "- Outlier detection and treatment (if needed)\n",
    "- Preparing data for modeling\n",
    "- Any other preprocessing steps (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUi6E9EUMKth"
   },
   "source": [
    "#### Missing Value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of the dataframe\n",
    "df2=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "There are 202 null values. There are many outliers in the data set, hence using the mean will definitely lead to some bias. I will be using the median to fill the null values for the columns based on the outliers observation from the box plots, i will group by release year and brandname and get the median of the 2 columns together to fill each of the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling null values for main camera column\n",
    "df2[\"main_camera_mp\"] = df2[\"main_camera_mp\"].fillna(\n",
    "    value=df2.groupby([\"brand_name\"])[\"main_camera_mp\"].transform(\"median\")\n",
    ")\n",
    "#Rechecking the null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are still some unfilled values from the main camera column, so i will fill them with the median of the main camera column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the null values for main camera column\n",
    "df2[\"main_camera_mp\"] = df2[\"main_camera_mp\"].fillna(df2[\"main_camera_mp\"].median())\n",
    "#rechecking the null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling null value for selfie camera\n",
    "df2[\"selfie_camera_mp\"] = df2[\"selfie_camera_mp\"].fillna(\n",
    "    value=df2.groupby([\"brand_name\"])[\"selfie_camera_mp\"].transform(\"median\")\n",
    ")\n",
    "#rechecking the null values\n",
    "df2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling null values for int memory column\n",
    "df2[\"int_memory\"] = df2[\"int_memory\"].fillna(\n",
    "    value=df2.groupby([\"release_year\",\"brand_name\"])[\"int_memory\"].transform(\"median\")\n",
    ")\n",
    "#rechecking the null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values for ram column with median\n",
    "df2[\"ram\"] = df2[\"ram\"].fillna(\n",
    "    value=df2.groupby([\"release_year\",\"brand_name\"])[\"ram\"].transform(\"median\")\n",
    ")\n",
    "#Rechecking the null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling null values for battery\n",
    "df2[\"battery\"] = df2[\"battery\"].fillna(\n",
    "#rechecking the null values\n",
    "    value=df2.groupby([\"brand_name\"])[\"battery\"].transform(\"median\")\n",
    ")\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling null values for weight\n",
    "df2[\"weight\"] = df2[\"weight\"].fillna(\n",
    "    value=df2.groupby([\"brand_name\"])[\"weight\"].transform(\"median\")\n",
    ")\n",
    "#rechecking the null values\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: All null values have now been filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm creating a new column that represents the number of years that have passed since the phones and tablets where released to replace release_year because the column 'releas_year' will vary from time to time. \n",
    "#The years to date column is more permanent and stable than the release_year column\n",
    "df2[\"years_to_date\"] = 2022 - df2[\"release_year\"]\n",
    "df2.drop(\"release_year\", axis=1, inplace=True)\n",
    "df2[\"years_to_date\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking out the data frame again\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The years to date column has now been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers in the data set by plotting a box plot for all numerical variables\n",
    "num_cols = df2.select_dtypes(include=np.number).columns.tolist()\n",
    "for i, variable in enumerate (num_cols):\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.boxplot(data=df2,x=variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. Screen size and Ram have upper and lower outliers\n",
    "2. Normalized new price and normalized used price have both upper and lower outliers\n",
    "3. weight has a lot of lower outliers\n",
    "4. main camera mp, selfie camera mp, int memory, battery and weight all have lower outliers\n",
    "5. There are no outliers in years to date and the days_used column.\n",
    "\n",
    "\n",
    "#### I will not be treating the outliers as they seem to be valuable in the model prediction.\n",
    "Since we are dealing with devices which include phones AND tablets, the outliers seem true. eg, tablets have relatively larger screen sizes than phones and tend to weigh much more.\n",
    "I will consider this in building my model, and would pay attention to the RMSE since the MAE may not consider outliers in the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the dependent and independent variables\n",
    "#I am dropping this column because it's the column for which we are checking dependency\n",
    "#independent variable\n",
    "\n",
    "x = df2.drop([\"normalized_used_price\"], axis=1)\n",
    "# dependent variable\n",
    "y = df2[[\"normalized_used_price\"]]\n",
    "\n",
    "print(x.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the intercept to data\n",
    "x = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dummy variables\n",
    "x = pd.get_dummies(\n",
    "    x,\n",
    "    columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n",
    "    drop_first=True,\n",
    ")\n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will now split the data in the 70:30 ratio for train to test data respectively\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the first few rows of the training data\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the first few rows of the testing data\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the number of rows and columns in both train and test data\n",
    "print(\"Number of rows in train data =\", x_train.shape[0], \"and the number of columns in train data =\", x_train.shape[1])\n",
    "print(\"Number of rows in test data =\", x_test.shape[0], \"and the number of columns in test data =\", x_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeUzI1OB4rqM"
   },
   "source": [
    "## Model Building - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNRiMg0wMKth"
   },
   "outputs": [],
   "source": [
    "#Building the model and getting the summary of the regression results\n",
    "olsmodel1 = sm.OLS(y_train,x_train).fit()\n",
    "print(olsmodel1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the rsquared\n",
    "print(\"The coefficient of determination (R-squared) is \", olsmodel1.rsquared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations Interpreting the Regression results\n",
    "\n",
    "Adjusted. R-squared:, the value for adj. R-squared is 0.845, which is good.\n",
    "\n",
    "The R-squared value tells us that our model can explain 84.5% of the variance in the training set.\n",
    "\n",
    "If all the predictor variable coefficients are zero, then the expected output (i.e., Y) would be equal to the const coefficient which in this case is 1.3393\n",
    "Coefficient of a predictor variable: It represents the change in the output Y due to a change in the predictor variable (everything else held constant).\n",
    "In our case, the coefficient of selfie_camera_mp is 0.0135."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvoU3F6oMKti"
   },
   "source": [
    "## Model Performance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3e6gzxdMKti"
   },
   "outputs": [],
   "source": [
    "#We can check the model performance by checking the rsquared, adjusted r squared, RMSE, MAE and MAPE \n",
    "#function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# function to compute MAPE\n",
    "def mape_score(targets, predictions):\n",
    "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    #computing r2,adjr2,rmse,mae and mape\n",
    "    r2 = r2_score(target, pred)  \n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  \n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  \n",
    "    mae = mean_absolute_error(target, pred) \n",
    "    mape = mape_score(target, pred)  \n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmodel1_train_perf = model_performance_regression(olsmodel1, x_train, y_train)\n",
    "olsmodel1_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmodel1_test_perf = model_performance_regression(olsmodel1,x_test,y_test)\n",
    "olsmodel1_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The testing R squared is 0.84, same with the training Rsquared so the model is not underfitting\n",
    "\n",
    "The RMSE of the testing data is slightly higher than the training data by 0.008474 so it seems the model is also slightly overfitting.\n",
    "\n",
    "MAE suggests that the model can predict anime ratings within a mean error of 0.18 on the test data\n",
    "\n",
    "Due to some of the cells legitimately having values as zero (eg some columns with devices which do not have selfie cameras), MAPE calculation has produced an infinite number which is being shown as NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9GxSQf-qH8e"
   },
   "source": [
    "## Checking Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-In order to make statistical inferences from a linear regression model, it is important to ensure that the assumptions of linear regression are satisfied.\n",
    "### The assumptions to be tested include:\n",
    "\n",
    "**1. No Multicollinearity between the predictor variables**\n",
    "\n",
    "**2. Linearity of variables**\n",
    "\n",
    "**3. Independence of error terms**\n",
    "\n",
    "**4. Normality of error terms**\n",
    "\n",
    "**5. No Heteroscedasticity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now check these assumptions one after the other.\n",
    "#To check for multicolinearity, we can check the VIF.\n",
    "#let's check the VIF of the predictors\n",
    "def checking_vif(predictors):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = predictors.columns\n",
    "\n",
    "    # calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(predictors.values, i)\n",
    "        for i in range(len(predictors.columns))\n",
    "    ]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Rule of Thumb:\n",
    "\n",
    "If VIF is 1, then there is no correlation among the  𝑘\n",
    "k\n",
    " th predictor and the remaining predictor variables, and hence, the variance of  𝛽𝑘\n",
    "β\n",
    "k\n",
    "  is not inflated at all.\n",
    "  \n",
    "  \n",
    "If VIF exceeds 5, we say there is moderate VIF, and if it is 10 or exceeding 10, it shows signs of high multi-collinearity.\n",
    "\n",
    "\n",
    "for the purpose of this analysis, we would check the variables with VIF above 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the VIF of the training data\n",
    "checking_vif(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "Based on th VIF scores, some of the dummy variables have VIF scores more than 5. however, since VIF scores for dummy variables does not really affect the model, i will not drop the columns yet. the columns 'screen_size' and 'weight' have VIF above 5, so we can look at dropping them one after the other and seeing the effect on the VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to check for the effect of dropping columns showing high multicolinearity:\n",
    "def treating_multicollinearity(predictors, target, high_vif_columns):\n",
    "    \"\"\"\n",
    "    Checking the effect of dropping the columns showing high multicollinearity\n",
    "    on model performance (adj. R-squared and RMSE)\n",
    "\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    high_vif_columns: columns having high VIF\n",
    "    \"\"\"\n",
    "    # empty lists to store adj. R-squared and RMSE values\n",
    "    adj_r2 = []\n",
    "    rmse = []\n",
    "\n",
    "    # build ols models by dropping one of the high VIF columns at a time\n",
    "    # store the adjusted R-squared and RMSE in the lists defined previously\n",
    "    for cols in high_vif_columns:\n",
    "        # defining the new train set\n",
    "        train = predictors.loc[:, ~predictors.columns.str.startswith(cols)]\n",
    "\n",
    "        # create the model\n",
    "        olsmodel = sm.OLS(target, train).fit()\n",
    "\n",
    "        # adding adj. R-squared and RMSE to the lists\n",
    "        adj_r2.append(olsmodel1.rsquared_adj)\n",
    "        rmse.append(np.sqrt(olsmodel1.mse_resid))\n",
    "\n",
    "    # creating a dataframe for the results\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            \"col\": high_vif_columns,\n",
    "            \"Adj. R-squared after_dropping col\": adj_r2,\n",
    "            \"RMSE after dropping col\": rmse,\n",
    "        }\n",
    "    ).sort_values(by=\"Adj. R-squared after_dropping col\", ascending=False)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the effect of dropping weight and screensize\n",
    "col_list=['screen_size','weight']\n",
    "res = treating_multicollinearity(x_train, y_train, col_list) \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "the above shows that the adjusted r squared wil not drop significantly after each of the columns are dropped. Thus, we can drop the columns, but i will drop them one after the other in case dropping one causes the VIF of the other to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the screen size column first since it has the highest VIF\n",
    "col_drop = 'screen_size' \n",
    "x_train2 = x_train.loc[:, ~x_train.columns.str.startswith(col_drop)]\n",
    "x_test2 = x_test.loc[:, ~x_test.columns.str.startswith(col_drop)]\n",
    "vif = checking_vif(x_train2)\n",
    "print(\"VIF after dropping \", col_drop)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "As suspected, after dropping the screen size column, the VIF for weight is now below 5. now, all the VIF scores are below 5, except for the dummy variables, which we will treat with pvalue check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the model again after dropping columns\n",
    "olsmodel2=sm.OLS(y_train,x_train2).fit()\n",
    "print(olsmodel2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naX-iXItqH-b"
   },
   "source": [
    "### Observation:\n",
    "\n",
    "There is no significant change in R squared and adjusted r squared since the screen size column was dropped.\n",
    "\n",
    "#### Checking the p-values\n",
    "Variables with High P values do not help our model, hence i will need to identify them and drop them.\n",
    "\n",
    "Setting the null and alternative hypothesis:\n",
    "\n",
    "Ho : Independent feature is not significant (βi=0 ) Ha : Independent feature is significant (βi≠0 ) (P>|t|) gives the p-value for each independent feature to check that null hypothesis. We are considering 0.05 (5%) as significance level.\n",
    "\n",
    "A p-value of less than 0.05 is considered to be statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting only the columns that fall within the range\n",
    "# initial list of columns\n",
    "predictors = x_train2.copy()\n",
    "cols = predictors.columns.tolist()\n",
    "\n",
    "# setting an initial max p-value\n",
    "max_p_value = 1\n",
    "\n",
    "while len(cols) > 0:\n",
    "    # defining the train set\n",
    "    x_train_aux = predictors[cols]\n",
    "\n",
    "    # fitting the model\n",
    "    model = sm.OLS(y_train, x_train_aux).fit()\n",
    "\n",
    "    # getting the p-values and the maximum p-value\n",
    "    p_values = model.pvalues\n",
    "    max_p_value = max(p_values)\n",
    "\n",
    "    # name of the variable with maximum p-value\n",
    "    feature_with_p_max = p_values.idxmax()\n",
    "\n",
    "    if max_p_value > 0.05:\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#getting the list of columns with pvalues in the given range:\n",
    "selected_features = cols\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new training and testing sets with only the selected features\n",
    "x_train3 = x_train2[selected_features]\n",
    "x_test3 = x_test2[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the new model summary\n",
    "olsmod3=sm.OLS(y_train,x_train3).fit()\n",
    "print(olsmod3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. All the variables now have p values less than 0.005.\n",
    "2. The difference between the r squared and asjusted rsquared has also decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmod3_train_perf = model_performance_regression(olsmod3, x_train3, y_train)\n",
    "olsmod3_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmod3_test_perf = model_performance_regression(olsmod3, x_test3, y_test)\n",
    "olsmod3_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: The Rsquared and adjusted rsquared are very similar for the both the test and training data. The RMSE and MAE is slightly higher in the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next assumption we will test for is linearity and independence\n",
    "#test for linearity and independence is done by plotting a residual plot.\n",
    "#creating a data frame with actual, fitted and residual values\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "df_pred[\"Actual Values\"] = y_train  \n",
    "df_pred[\"Fitted Values\"] = olsmod3.fittedvalues  \n",
    "df_pred[\"Residuals\"] = olsmod3.resid  \n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the residual plot with the actual, fitted and residual values\n",
    "sns.residplot(\n",
    "    data=df_pred, x=\"Fitted Values\", y=\"Residuals\", color=\"purple\", lowess=True\n",
    ")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Fitted vs Residual plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "In the above plot, i do not see any pattern.It looks random, therefore the assumptions of linearity and independence are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next assumption we will check for is normality. we can check for normality by plotting a histogram of the residuals, q-q plot and shapiro wilks test.\n",
    "#Checking for normality by plotting a histogram of the residuals\n",
    "sns.histplot(data=df_pred, x=\"Residuals\", kde=True)\n",
    "plt.title(\"Normality of residuals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "the histogram of the residuals have a bell shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for normality using the qq plot:\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(df_pred[\"Residuals\"], dist=\"norm\", plot=pylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for normality using Shapiro Wilk's test\n",
    "stats.shapiro(df_pred[\"Residuals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. from the qq plot, we can see that most of the residuals more or less follow a straight line except for some at the beginning and at the end\n",
    "2. The shapiro test results show a very low p value, meaning that the residuals are not normal However, as a consideration which is based on the distribution and qq plot, we can say it is a good approximation of the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The next assumption we will test for is Homoscedasticity\n",
    "I will use The goldfeldquandt test to check for Homoscedasticity.\n",
    "first, we will need to define the null and alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stating the Null and alternative hypothesis for the Goldfeldquandt test:\n",
    "\n",
    "**Null hypothesis: Residuals are homoscedastic**\n",
    "\n",
    "**Alternate hypothesis: Residuals have heteroscedasticity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for homoscedasticity with the goldfeldquandt test\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "name = [\"F statistic\", \"p-value\"]\n",
    "test = sms.het_goldfeldquandt(df_pred[\"Residuals\"], x_train3)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation: \n",
    "Since p-value > 0.05, we can say that the residuals are homoscedastic. So, this assumption is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have tested all the assumptions on our model, we can now build the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRYSDgFZMKtm"
   },
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Sqvs4TMKtn"
   },
   "outputs": [],
   "source": [
    "#Building the final model:\n",
    "olsmodel_final = sm.OLS(y_train, x_train3).fit()\n",
    "print(olsmodel_final.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the parameters and their coefficients which would make up the equation of linear regression:\n",
    "olsmodel_final.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the equation of linear regression\n",
    "Equation = \"normalized_used_price =\"\n",
    "print(Equation, end=\" \")\n",
    "for i in range(len(x_train3.columns)):\n",
    "    if i == 0:\n",
    "        print(olsmodel_final.params[i], \"+\", end=\" \")\n",
    "    elif i != len(x_train3.columns) - 1:\n",
    "        print(\n",
    "            olsmodel_final.params[i],\n",
    "            \"* (\",\n",
    "            x_train3.columns[i],\n",
    "            \")\",\n",
    "            \"+\",\n",
    "            end=\"  \",\n",
    "        )\n",
    "    else:\n",
    "        print(olsmodel_final.params[i], \"* (\", x_train3.columns[i], \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Equation for Linear Regression\n",
    "\n",
    "The constant is 1.529. This means that if all other predictor variables are equal to zero, the normalized used price will be 1.529\n",
    "\n",
    "A unit increase in main camera MP increases the normalized used price by ~0.021 units, all other variables held constant.\n",
    "\n",
    "A unit increase in selfie camera MP increases the normalized used price by ~0.013 units, all other variables held constant.\n",
    "\n",
    "A unit increase in RAM increases the normalized used price by ~0.021 units, all other variables held constant.\n",
    "\n",
    "A unit increase in weight increases the normalized used price by 0.002 units, all other variables held constant.\n",
    "\n",
    "A unit increase in normalized new price increases the normalized used price by ~0.442 units, all other variables held constant.\n",
    "\n",
    "A unit increase in years to date decreases the normalized used price by ~0.029 units, all other variables held constant.\n",
    "\n",
    "A unit increase in brand name Karbonn increases the normalized used price by ~0.116 units, all other variables held constant.\n",
    "\n",
    "A unit increase in brand name Samsung decreases the normalized used price by ~0.037 units, all other variables held constant.\n",
    "\n",
    "A unit increase in brand name Sony decreases the normalized used price by ~0.067 units, all other variables held constant.\n",
    "\n",
    "A unit increase in brand name Xiaomi increases the normalized used price by ~0.080 units, all other variables held constant.\n",
    "\n",
    "A unit increase os Others decreases the normalized used price by ~0.128 units, all other variables held constant.\n",
    "\n",
    "A unit increase in Os_ios decreases the normalized used price by ~0.090 units, all other variables held constant.\n",
    "\n",
    "A unit increase in 4g_yes increases the normalized used price by ~0.050 units, all other variables held constant.\n",
    "\n",
    "A unit increase in 5g_yes decreases the normalized used price by ~0.067 units, all other variables held constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the above model to make predictions on the data\n",
    "#Checking the model perfomance on the training data:\n",
    "# checking model performance on train set (seen 70% data)\n",
    "print(\"Training Performance\\n\")\n",
    "olsmodel_final_train_perf = model_performance_regression(olsmodel_final,x_train3,y_train) ## Complete the code to check the performance on train data\n",
    "olsmodel_final_train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model performance on test set (seen 30% data)\n",
    "print(\"Test Performance\\n\")\n",
    "olsmodel_final_test_perf = model_performance_regression(\n",
    "    olsmodel_final, x_test3, y_test\n",
    ")\n",
    "olsmodel_final_test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "The model is able to explain ~84% of the variation in the data\n",
    "\n",
    "The train and test RMSE and MAE are low and comparable. So, our model is not suffering from overfitting\n",
    "\n",
    "The rsquared and adjusted rsquared is comparable between the test set and in the training set, hence we can say that our model performs well on both our training and testing data and is not underfitting\n",
    "\n",
    "MAE suggests that the model can predict normalized used price within a mean error of ~0.19 on the test data\n",
    "\n",
    "We can conclude the model olsmodel_final is good for prediction as well as inference purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BkZh6eHluZK"
   },
   "source": [
    "## Actionable Insights and Recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh_zkgqs4rqN"
   },
   "source": [
    "- These variables correlate with the normalized used price positively: screen_size, main_camera_mp, Selfie_camera_mp, weight, ram, normalized new price, brand name_Karbonn, brand_name Xiaomi and 4g_yes.\n",
    "\n",
    "- These variables correlate with the normalized used price negatively: years to date, brand name Samsung, brand name Sony, os others, os Ios and 5g_yes.\n",
    "\n",
    "- The variable that has the most significant effect on the normalized used price is the normalized new price. The higher the price of a device when new, the higher the price when used, thus Recell can look to add more devices of this nature in their portal.\n",
    "\n",
    "- It will be more profitable for Recell to avoid devices with Os in the others category and Ios since these will decrease the used price. Android and Microsoft OS should be prioritized against others.\n",
    "\n",
    "- Since devices that have higher RAMs will increase the normalized used price, brand names like Oneplus, Oppo , Huawei and Vevo which have higher rams would have increased prices\n",
    "\n",
    "- Brand names like Samsung and Sony should be avoided as the used price is lower than other brands. Recell should rather go for more of the other brands especially devices from Karbonn and Xiaomi as they have a positive effect on the used device prices.\n",
    "\n",
    "- devices with larger main and selfie camera mp sell for higher prices than the devices with lower megapixels, so Recell will need to consider this while accepting used devices to sell. customers who love selfies can choose from a range of Huawei, Oppo, Vivo and Xiaomi devices as they have the highest number of options with selfie camera mp.\n",
    "\n",
    "- Recell can avoid 5g enabled devices since they decrease the used prices. 4g enabled devices will be more profitable\n",
    "\n",
    "- Devices that weigh more are sold for higher prices, and we can see from the visualization that battery size and possibly screensize affects the weight of devices. Recell can choose devices that weigh more than those that weigh less. some of the brands in this category are Google, HTC and Lenovo\n",
    "\n",
    "- Recell can run a promotion to phase out all devices in their stock with longer years to date so that they can focus more on the recent devices. This is because phones with less years to date (phones released more recently) are preferrable since the longer the phone has been released, the lower the used price.\n",
    "\n",
    "- The most profitable devices at Recell will be the 4g enabled devices manufactured more recently which have higher selfie and main camera mega pixels, ram and weight. I will recommend that Recell stocks these devices for sale. Preferably, they should be from Karbonn, xiaomi and other brands except Sony and samsung, and would have either Android or windows operating system.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWXXovuh4rqN"
   },
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3SntBY974rqJ",
    "v_-uuGqH-qTt",
    "xxhpZv9y-qTw",
    "UvpMDcaaMKtI",
    "__7ciGcIDPyk",
    "pVn5toJ7MKte",
    "KNzFis7eEaXj",
    "HeUzI1OB4rqM",
    "jvoU3F6oMKti",
    "a9GxSQf-qH8e",
    "jRYSDgFZMKtm",
    "2BkZh6eHluZK"
   ],
   "name": "SLF_Project_LearnerNotebook_FullCode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
